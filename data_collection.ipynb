{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Process of Data Collection & Conversion to .csv File**\n",
    "\n",
    "In this process, we outline the code steps taken to collect the data and convert it to .csv files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These files were then read, cleaned, and manipulated in the phase 2 submission file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Player Stats**\n",
    "In this section after importing above, I collected the url and created a dataframe that collected all of the data from all of the seasons from the 2003-2004 season to the 2022-2023 season for the top player stats, ranking which players led during the season and what their stats were. I combined all of this data into one main data frame using a for loop. The data is originally filtered by average points per game for each player per season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list of all of the seasons to collect data from\n",
    "season_years = [\"2003-04\", \"2004-05\", \"2005-06\", \"2006-07\", \"2007-08\", \"2008-09\", \"2009-10\", \"2010-11\", \\\n",
    "    \"2012-13\", \"2013-14\", \"2014-15\", \"2015-16\", \"2016-17\", \"2017-18\", \"2018-19\", \"2019-20\", \"2020-21\", \\\n",
    "    \"2021-22\",\"2022-23\"]\n",
    "\n",
    "#Intialize empty list for dataframes to be added to\n",
    "dataframes = []\n",
    "\n",
    "#Loop through the season years and \n",
    "for season_year in season_years:\n",
    "\n",
    "    #Add {season_year} into the api url to change each iteration\n",
    "    url = f\"https://stats.nba.com/stats/leagueLeaders?LeagueID=00&PerMode=PerGame&Scope=S&Season={season_year}&SeasonType=Regular%20Season&StatCategory=PTS\"\n",
    "    response = requests.get(url).json()\n",
    "\n",
    "    table_headers = response['resultSet']['headers']\n",
    "    season_data = pd.DataFrame(response['resultSet']['rowSet'], columns=table_headers)\n",
    "\n",
    "    # Do we want this to be first or last?\n",
    "    season_data['Year'] = season_year\n",
    "\n",
    "    # Append the dataframe to the list\n",
    "    dataframes.append(season_data)\n",
    "\n",
    "# Concatenate all dataframes into one giant dataframe\n",
    "player_stat_df = pd.concat(dataframes, ignore_index=True)\n",
    "player_stat_df\n",
    "#player_stat_df.to_csv('player_stats.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Team Stat Data Collection**\n",
    "@Akhil add in process here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_years = [\"2003\", \"2004\", \"2005\", \"2006\", \"2007\", \\\n",
    "\"2008\", \"2009\", \"2010\", \"2012\", \"2013\", \"2014\", \\\n",
    "    \"2015\", \"2016\", \"2017\", \"2018\", \"2019\", \"2020\", \\\n",
    "        \"2021\",\"2022\"]\n",
    "season = '2003'\n",
    "url = f\"https://www.basketball-reference.com/leagues/NBA_{season}.html\"\n",
    "\n",
    "result = requests.get(url)\n",
    "if result.status_code != 200:\n",
    "    print(\"Something went wrong:\", result.status_code, result.reason)\n",
    "\n",
    "with open(\"nba_teams.html\", \"w\", encoding='utf-8') as writer:\n",
    "    writer.write(result.text)\n",
    "\n",
    "with open(\"nba_teams.html\", \"r\", encoding='utf-8') as reader:\n",
    "    html_source = reader.read()\n",
    "\n",
    "page = BeautifulSoup(html_source, \"html.parser\")\n",
    "\n",
    "\n",
    "team_name_lst = page.find_all(\"th\", {'data-stat':'team_name'})\n",
    "team_names = []\n",
    "\n",
    "for x in team_name_lst:\n",
    "    team_names.append(x.text)\n",
    "\n",
    "team_names.remove(\"Eastern Conference\")\n",
    "team_names.remove(\"Western Conference\")\n",
    "\n",
    "\n",
    "lst = ['wins', 'losses', 'win_loss_pct', 'pts_per_g', 'opp_pts_per_g', 'srs']\n",
    "result = [team_names]\n",
    "for x in lst:\n",
    "    tmp = page.find_all(\"td\", {'data-stat': f'{x}'})\n",
    "    tmp_lst = []\n",
    "    for y in tmp:\n",
    "        if len(tmp_lst) >= len(team_names):\n",
    "            break\n",
    "        tmp_lst.append(y.text)\n",
    "    result.append(tmp_lst)\n",
    "\n",
    "df= pd.DataFrame(result)\n",
    "\n",
    "df.columns = team_names\n",
    "df = df[1:]\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.index = [\"w\", \"l\", 'w/l %', 'avg pts', 'opp avg pts', 'srs' ]\n",
    "df = df.T\n",
    "#df.to_csv('team_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Historical MVP Data Collection**\n",
    "This is the data historically ranking the MVPs over the seasons that are being analyzed. It goes through the list of target seasons and splits the text on the page to create a dataframe. This had to be done by using a text representation since the other sources were not able to be scraped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.nba.com/news/history-mvp-award-winners\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code != 200:\n",
    "        print(\"Something went wrong:\", response.status_code, response.reason)\n",
    "\n",
    "page = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "\n",
    "target_seasons = [\"2003-04\", \"2004-05\", \"2005-06\", \"2006-07\", \"2007-08\", \"2008-09\", \"2009-10\", \\\n",
    "    \"2010-11\", \"2011-12\", \"2012-13\", \"2013-14\", \"2014-15\", \"2015-16\", \"2016-17\", \"2017-18\", \\\n",
    "    \"2018-19\", \"2019-20\", \"2020-21\", \"2021-22\", \"2022-23\"]\n",
    "\n",
    "data = {} \n",
    "\n",
    "# Loop through the target seasons\n",
    "for target_season in target_seasons:\n",
    "    for p_tag in page.find_all('p'):\n",
    "        if target_season in p_tag.text:\n",
    "            split_data = p_tag.text.split(' â€” ')\n",
    "            if len(split_data) > 1:\n",
    "                winner = split_data[1].split(',')[0]\n",
    "                data[target_season] = winner\n",
    "\n",
    "# Create a DataFrame from the data dictionary\n",
    "mvp_df = pd.DataFrame(list(data.items()), columns=[\"Year\", \"MVP_Name\"])\n",
    "mvp_df\n",
    "#mvp_df.to_csv('mvp_historical.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 ('info2950')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4f30da6fd141dcec1f686a2d3870ea3fbe3345d19df0b7c05e23f27c1e7c07e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
